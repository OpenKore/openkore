# =============================================================================
# AI Sidecar Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment
# All values can be overridden via environment variables
# =============================================================================

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------

# Application name for logging and identification
# Required: No | Default: AI-Sidecar
AI_APP_NAME=AI-Sidecar

# Enable debug mode (increases logging verbosity and enables dev features)
# Required: No | Default: false
AI_DEBUG=false

# -----------------------------------------------------------------------------
# ZeroMQ IPC Configuration
# -----------------------------------------------------------------------------

# ZMQ socket endpoint (REP socket listens here)
# Required: Yes | Default: tcp://127.0.0.1:5555
# Examples: tcp://127.0.0.1:5555, ipc:///tmp/openkore-ai.sock
AI_ZMQ_ENDPOINT=tcp://127.0.0.1:5555

# Socket receive timeout in milliseconds (0 = blocking)
# Required: No | Default: 100
AI_ZMQ_RECV_TIMEOUT_MS=100

# Socket send timeout in milliseconds
# Required: No | Default: 100
AI_ZMQ_SEND_TIMEOUT_MS=100

# High water mark for outgoing messages (0 = unlimited)
# Required: No | Default: 1000
AI_ZMQ_HIGH_WATER_MARK=1000

# Socket linger time in milliseconds on close
# Required: No | Default: 0
AI_ZMQ_LINGER_MS=0

# -----------------------------------------------------------------------------
# Tick Processor Configuration
# -----------------------------------------------------------------------------

# Tick interval in milliseconds (100ms = 10 ticks/second)
# Required: No | Default: 100
AI_TICK_INTERVAL_MS=100

# Maximum processing time per tick before warning (ms)
# Required: No | Default: 80
# Note: Must be less than AI_TICK_INTERVAL_MS
AI_TICK_MAX_PROCESSING_MS=80

# Number of state history entries to maintain
# Required: No | Default: 100
AI_TICK_STATE_HISTORY_SIZE=100

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Required: No | Default: INFO
# Use DEBUG for troubleshooting, INFO for production
AI_LOG_LEVEL=INFO

# Log output format: json, console, text
# Required: No | Default: console
# Use 'console' for humans, 'json' for log aggregators
AI_LOG_FORMAT=console

# Log file path (leave empty for stdout only)
# Required: No | Default: None
# Example: logs/ai_sidecar.log
AI_LOG_FILE_PATH=

# Include timestamp in logs
# Required: No | Default: true
AI_LOG_INCLUDE_TIMESTAMP=true

# Include file:line caller information in logs (useful for debugging)
# Required: No | Default: false
AI_LOG_INCLUDE_CALLER=false

# -----------------------------------------------------------------------------
# Decision Engine Configuration
# -----------------------------------------------------------------------------

# AI engine type: stub, rule_based, ml
# Required: No | Default: rule_based
# - stub: Returns empty decisions (for testing)
# - rule_based: CPU-based rule engine (recommended for most users)
# - ml: Machine learning engine (requires good CPU/GPU)
AI_DECISION_ENGINE_TYPE=rule_based

# Fallback mode when ML is unavailable: cpu, idle, defensive
# Required: No | Default: cpu
# - cpu: Use rule-based AI (recommended)
# - idle: Do nothing (safe but not useful)
# - defensive: Only defensive actions
AI_DECISION_FALLBACK_MODE=cpu

# Maximum actions per tick/decision cycle
# Required: No | Default: 5
# Higher values = more aggressive but may look bot-like
AI_DECISION_MAX_ACTIONS_PER_TICK=5

# Minimum confidence threshold for actions (0.0 - 1.0)
# Required: No | Default: 0.5
# Lower = more actions but more mistakes
AI_DECISION_MIN_CONFIDENCE=0.5

# -----------------------------------------------------------------------------
# Health Check Configuration
# -----------------------------------------------------------------------------

# Health check interval in seconds
# Required: No | Default: 5.0
# Lower = faster detection of issues, higher CPU usage
AI_HEALTH_CHECK_INTERVAL_S=5.0

# -----------------------------------------------------------------------------
# Compute Backend Configuration
# -----------------------------------------------------------------------------

# Primary compute backend: cpu, gpu, ml, llm
# Required: No | Default: cpu
# - cpu: Rule-based AI (works on any hardware, fast)
# - gpu: Neural network AI (requires NVIDIA CUDA GPU)
# - ml: Machine learning AI (requires good CPU/GPU)
# - llm: LLM-powered AI (requires API key, slower but intelligent)
COMPUTE_BACKEND=cpu

# =============================================================================
# MEMORY AND LEARNING SYSTEMS
# =============================================================================

# -----------------------------------------------------------------------------
# Persistent Memory Configuration
# -----------------------------------------------------------------------------

# SQLite database path for long-term memory storage
# Required: No | Default: data/memory.db
AI_MEMORY_DB_PATH=data/memory.db

# Memory consolidation interval in seconds
# Required: No | Default: 300 (5 minutes)
# How often to consolidate short-term memory into long-term storage
AI_MEMORY_CONSOLIDATION_INTERVAL_S=300

# Maximum working memory buffer size (number of items)
# Required: No | Default: 1000
AI_MEMORY_WORKING_MAX_SIZE=1000

# Session memory TTL (time-to-live) in hours
# Required: No | Default: 24
# How long to keep session data before cleanup
AI_MEMORY_SESSION_TTL_HOURS=24

# -----------------------------------------------------------------------------
# Session Memory (Redis/DragonflyDB) - Optional
# -----------------------------------------------------------------------------

# Redis/DragonflyDB connection URL (optional - improves performance)
# Required: No | Default: None (uses in-memory fallback)
# If not set, session memory will use working memory only
# Examples:
#   redis://localhost:6379
#   redis://localhost:6379/0
#   redis://:password@localhost:6379/0
#   redis://username:password@localhost:6379
REDIS_URL=

# =============================================================================
# LLM PROVIDER CONFIGURATION (Optional - for LLM-powered features)
# =============================================================================
# Configure at least one provider for LLM-assisted decision-making and chat
# Providers are tried in order: OpenAI → Azure → DeepSeek → Claude
# Only needed if COMPUTE_BACKEND=llm or for advanced chat features

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------

# OpenAI API key (get from: https://platform.openai.com/api-keys)
# Required: Only if using OpenAI | Default: None
OPENAI_API_KEY=

# OpenAI model to use
# Required: No | Default: gpt-4o-mini
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Cost: gpt-4o-mini is cheapest, gpt-4o is most capable
OPENAI_MODEL=gpt-4o-mini

# OpenAI API base URL (for custom endpoints)
# Required: No | Default: https://api.openai.com/v1
OPENAI_API_BASE=

# -----------------------------------------------------------------------------
# Azure OpenAI Configuration
# -----------------------------------------------------------------------------

# Azure OpenAI API key
# Required: Only if using Azure OpenAI | Default: None
AZURE_OPENAI_KEY=

# Azure OpenAI endpoint URL
# Required: Only if using Azure OpenAI | Default: None
# Example: https://your-resource.openai.azure.com/
AZURE_OPENAI_ENDPOINT=

# Azure OpenAI deployment name
# Required: Only if using Azure OpenAI | Default: gpt-4
# This is the deployment name you created in Azure Portal
AZURE_OPENAI_DEPLOYMENT=gpt-4

# Azure OpenAI API version
# Required: No | Default: 2024-02-01
AZURE_OPENAI_API_VERSION=2024-02-01

# -----------------------------------------------------------------------------
# DeepSeek Configuration (Cost-Effective Alternative)
# -----------------------------------------------------------------------------

# DeepSeek API key (get from: https://platform.deepseek.com/)
# Required: Only if using DeepSeek | Default: None
# Note: DeepSeek is ~70% cheaper than OpenAI with comparable quality
DEEPSEEK_API_KEY=

# DeepSeek model to use
# Required: No | Default: deepseek-chat
# Options: deepseek-chat, deepseek-coder
DEEPSEEK_MODEL=deepseek-chat

# DeepSeek API base URL
# Required: No | Default: https://api.deepseek.com/v1
DEEPSEEK_API_BASE=

# -----------------------------------------------------------------------------
# Claude (Anthropic) Configuration
# -----------------------------------------------------------------------------

# Anthropic API key (get from: https://console.anthropic.com/)
# Required: Only if using Claude | Default: None
ANTHROPIC_API_KEY=

# Claude model to use
# Required: No | Default: claude-3-haiku-20240307
# Options: claude-3-haiku-20240307 (fast/cheap), claude-3-sonnet-20240229 (balanced), claude-3-opus-20240229 (most capable)
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Anthropic API base URL (for custom endpoints)
# Required: No | Default: https://api.anthropic.com
ANTHROPIC_API_BASE=

# -----------------------------------------------------------------------------
# Google AI (Gemini) Configuration - Optional
# -----------------------------------------------------------------------------

# Google AI API key (get from: https://makersuite.google.com/app/apikey)
# Required: Only if using Google AI | Default: None
GOOGLE_API_KEY=

# Google AI model to use
# Required: No | Default: gemini-pro
# Options: gemini-pro, gemini-pro-vision
GOOGLE_MODEL=gemini-pro

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLM) - Optional
# -----------------------------------------------------------------------------

# Ollama API base URL (for local LLM server)
# Required: Only if using Ollama | Default: None
# Example: http://localhost:11434
OLLAMA_API_BASE=

# Ollama model to use
# Required: Only if using Ollama | Default: llama2
# Must match model name in your Ollama installation
OLLAMA_MODEL=llama2

# =============================================================================
# LEARNING ENGINE CONFIGURATION
# =============================================================================

# Learning rate for strategy updates (0.0 - 1.0)
# Required: No | Default: 0.1
# Higher = faster learning but less stable, Lower = slower but more stable
AI_LEARNING_RATE=0.1

# Batch size for experience replay
# Required: No | Default: 20
# Number of experiences to learn from in each replay cycle
AI_LEARNING_BATCH_SIZE=20

# Outcome timeout in minutes
# Required: No | Default: 5
# Decisions older than this are marked as failed/timed out
AI_LEARNING_OUTCOME_TIMEOUT_MIN=5

# Experience replay interval in game ticks
# Required: No | Default: 1000
# How often to replay and learn from past experiences (every N ticks)
AI_LEARNING_REPLAY_INTERVAL_TICKS=1000

# =============================================================================
# SECURITY AND API RATE LIMITING
# =============================================================================

# API rate limit (requests per minute)
# Required: No | Default: 60
# Prevents excessive API calls to LLM providers
API_RATE_LIMIT_PER_MINUTE=60

# API daily budget limit in USD (0 = unlimited)
# Required: No | Default: 0
# Set this to prevent unexpected API costs
API_DAILY_BUDGET_LIMIT_USD=0

# Enable response caching to reduce API calls
# Required: No | Default: true
API_ENABLE_RESPONSE_CACHE=true

# Cache TTL in seconds
# Required: No | Default: 3600 (1 hour)
API_CACHE_TTL_SECONDS=3600

# =============================================================================
# ANTI-DETECTION CONFIGURATION
# =============================================================================

# Enable anti-detection features
# Required: No | Default: true
# Randomizes timing and behavior to appear more human-like
ANTI_DETECTION_ENABLED=true

# Paranoia level: low, medium, high, extreme
# Required: No | Default: medium
# Higher levels add more randomization but may reduce efficiency
ANTI_DETECTION_PARANOIA_LEVEL=medium

# Timing variance in milliseconds
# Required: No | Default: 150
# Random delay added to actions (Gaussian distribution)
ANTI_DETECTION_TIMING_VARIANCE_MS=150

# Maximum continuous play hours
# Required: No | Default: 4
# Bot will take breaks after this duration
ANTI_DETECTION_MAX_CONTINUOUS_HOURS=4

# Daily maximum play hours
# Required: No | Default: 12
# Total play time limit per 24-hour period
ANTI_DETECTION_DAILY_MAX_HOURS=12

# Enable GM detection and stealth mode
# Required: No | Default: true
# Automatically enters stealth mode when GM is detected nearby
ANTI_DETECTION_GM_DETECTION_ENABLED=true

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Enable performance profiling
# Required: No | Default: false
# Useful for debugging performance issues
AI_ENABLE_PROFILING=false

# Profiling output directory
# Required: No | Default: profiles
AI_PROFILE_OUTPUT_DIR=profiles

# Enable metrics collection (Prometheus-compatible)
# Required: No | Default: false
AI_ENABLE_METRICS=false

# Metrics export port (if metrics enabled)
# Required: No | Default: 9090
AI_METRICS_PORT=9090

# Maximum memory usage in MB (0 = unlimited)
# Required: No | Default: 512
# Process will try to stay under this limit
AI_MAX_MEMORY_MB=512

# =============================================================================
# OPENKORE INTEGRATION
# =============================================================================

# OpenKore control directory path
# Required: No | Default: None
# Path to OpenKore's control directory for reading config files
OPENKORE_CONTROL_DIR=

# OpenKore plugin directory path
# Required: No | Default: None
# Path to OpenKore's plugins directory
OPENKORE_PLUGIN_DIR=

# Enable OpenKore state synchronization
# Required: No | Default: true
# Keeps AI state in sync with OpenKore's game state
OPENKORE_STATE_SYNC_ENABLED=true

# =============================================================================
# END OF CONFIGURATION
# =============================================================================
# For more information, see:
# - README.md: https://github.com/OpenKore/openkore
# - Documentation: docs/GODTIER-AI-SPECIFICATION.md
# - AI Sidecar README: ai_sidecar/README.md