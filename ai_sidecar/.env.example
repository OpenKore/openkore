# =============================================================================
# AI Sidecar Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment
# All values can be overridden via environment variables
# =============================================================================

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------

# Application name for logging and identification
# Required: No | Default: AI-Sidecar
AI_APP_NAME=AI-Sidecar

# Enable debug mode (increases logging verbosity and enables dev features)
# Required: No | Default: false
AI_DEBUG=false

# -----------------------------------------------------------------------------
# ZeroMQ IPC Configuration - Inter-Process Communication
# -----------------------------------------------------------------------------

# ============================================================================
# ZMQ Socket Endpoint (where AI Sidecar listens for OpenKore messages)
# ============================================================================
#
# üåç AUTOMATIC PLATFORM DETECTION (Recommended - Leave Unset):
# ----------------------------------------------------------------------------
# The system automatically chooses the best endpoint for your platform:
#   ‚Ä¢ Windows/WSL1/Cygwin ‚Üí tcp://127.0.0.1:5555 (TCP - only option)
#   ‚Ä¢ Linux/macOS/WSL2    ‚Üí ipc:///tmp/openkore-ai.sock (IPC - faster)
#   ‚Ä¢ Docker              ‚Üí ipc:///tmp/openkore-ai.sock (Unix socket)
#
# ‚úÖ RECOMMENDED: Leave this commented out to use automatic detection
# ----------------------------------------------------------------------------
#
# üîß MANUAL OVERRIDE (Optional - Advanced Users):
# ----------------------------------------------------------------------------
# Uncomment and set to manually specify an endpoint:
#
# IPC Socket Examples (Unix/Linux/macOS/WSL2 only - fastest option):
#   AI_ZMQ_ENDPOINT=ipc:///tmp/openkore-ai.sock
#   AI_ZMQ_ENDPOINT=ipc:///var/run/openkore/ai.sock
#
# TCP Examples (works on all platforms):
#   AI_ZMQ_ENDPOINT=tcp://127.0.0.1:5555        # Local only (recommended)
#   AI_ZMQ_ENDPOINT=tcp://0.0.0.0:5555          # All interfaces (security risk!)
#   AI_ZMQ_ENDPOINT=tcp://192.168.1.100:5555    # Specific IP address
#
# ‚ö†Ô∏è IMPORTANT NOTES:
# ----------------------------------------------------------------------------
# 1. Both OpenKore (Perl) and AI Sidecar (Python) must use matching endpoints
#    - Check plugins/AI_Bridge/AI_Bridge.txt on the Perl side
#    - Both must point to same address/port
#
# 2. IPC sockets do NOT work on Windows
#    - Windows users MUST use TCP endpoint
#    - System auto-detects this and prevents IPC on Windows
#
# 3. Security consideration for TCP:
#    - Use 127.0.0.1 for same-machine only (secure)
#    - Use 0.0.0.0 ONLY if you need remote connections
#    - Never expose 0.0.0.0 to untrusted networks without firewall!
#
# üìä Performance Notes:
# ----------------------------------------------------------------------------
# IPC sockets:    <1ms latency    (fastest - Unix only)
# TCP localhost:  1-2ms latency   (good - all platforms)
# TCP remote:     5-50ms latency  (depends on network)
# ----------------------------------------------------------------------------
#
# üîç Troubleshooting:
# ----------------------------------------------------------------------------
# ‚Ä¢ Windows "IPC not supported" ‚Üí Remove this setting or use TCP
# ‚Ä¢ Unix "Address in use"       ‚Üí System auto-cleans stale sockets
# ‚Ä¢ Connection failed           ‚Üí Check both sides use same endpoint
# ‚Ä¢ See docs/ZMQ_TROUBLESHOOTING.md for detailed help
# ----------------------------------------------------------------------------
#
# AI_ZMQ_ENDPOINT=tcp://127.0.0.1:5555

# Socket receive timeout in milliseconds
# Required: No | Default: 100
# How long to wait for messages before timing out
# Increase if you see timeout errors in logs
AI_ZMQ_RECV_TIMEOUT_MS=100

# Socket send timeout in milliseconds
# Required: No | Default: 100
# How long to wait when sending messages before timing out
# Increase if you see send timeout errors
AI_ZMQ_SEND_TIMEOUT_MS=100

# High water mark for outgoing messages (0 = unlimited)
# Required: No | Default: 1000
# Message queue size before ZMQ starts dropping messages
# Increase for high-load scenarios, decrease to limit memory
AI_ZMQ_HIGH_WATER_MARK=1000

# Socket linger time in milliseconds on close
# Required: No | Default: 0
# 0 = close immediately (recommended for clean shutdowns)
# >0 = wait N milliseconds for pending messages before closing
AI_ZMQ_LINGER_MS=0

# -----------------------------------------------------------------------------
# Tick Processor Configuration
# -----------------------------------------------------------------------------

# Tick interval in milliseconds (100ms = 10 ticks/second)
# Required: No | Default: 100
AI_TICK_INTERVAL_MS=100

# Maximum processing time per tick before warning (ms)
# Required: No | Default: 80
# Note: Must be less than AI_TICK_INTERVAL_MS
AI_TICK_MAX_PROCESSING_MS=80

# Number of state history entries to maintain
# Required: No | Default: 100
AI_TICK_STATE_HISTORY_SIZE=100

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Required: No | Default: INFO
# Use DEBUG for troubleshooting, INFO for production
AI_LOG_LEVEL=INFO

# Log output format: json, console, text
# Required: No | Default: console
# Use 'console' for humans, 'json' for log aggregators
AI_LOG_FORMAT=console

# Log file path (leave empty for stdout only)
# Required: No | Default: logs/ai_sidecar.log
# Set to empty string to disable file logging and use stdout only
AI_LOG_FILE_PATH=logs/ai_sidecar.log

# Include timestamp in logs
# Required: No | Default: true
AI_LOG_INCLUDE_TIMESTAMP=true

# Include file:line caller information in logs (useful for debugging)
# Required: No | Default: false
AI_LOG_INCLUDE_CALLER=false

# -----------------------------------------------------------------------------
# Decision Engine Configuration
# -----------------------------------------------------------------------------

# AI engine type: stub, rule_based, ml
# Required: No | Default: rule_based
# - stub: Returns empty decisions (for testing)
# - rule_based: CPU-based rule engine (recommended for most users)
# - ml: Machine learning engine (requires good CPU/GPU)
AI_DECISION_ENGINE_TYPE=rule_based

# Fallback mode when ML is unavailable: cpu, idle, defensive
# Required: No | Default: cpu
# - cpu: Use rule-based AI (recommended)
# - idle: Do nothing (safe but not useful)
# - defensive: Only defensive actions
AI_DECISION_FALLBACK_MODE=cpu

# Maximum actions per tick/decision cycle
# Required: No | Default: 5
# Higher values = more aggressive but may look bot-like
AI_DECISION_MAX_ACTIONS_PER_TICK=5

# Minimum confidence threshold for actions (0.0 - 1.0)
# Required: No | Default: 0.5
# Lower = more actions but more mistakes
AI_DECISION_MIN_CONFIDENCE=0.5

# -----------------------------------------------------------------------------
# Health Check Configuration
# -----------------------------------------------------------------------------

# Health check interval in seconds
# Required: No | Default: 5.0
# Lower = faster detection of issues, higher CPU usage
AI_HEALTH_CHECK_INTERVAL_S=5.0

# -----------------------------------------------------------------------------
# Compute Backend Configuration
# -----------------------------------------------------------------------------

# Primary compute backend: cpu, gpu, ml, llm
# Required: No | Default: cpu
# - cpu: Rule-based AI (works on any hardware, fast)
# - gpu: Neural network AI (requires NVIDIA CUDA GPU)
# - ml: Machine learning AI (requires good CPU/GPU)
# - llm: LLM-powered AI (requires API key, slower but intelligent)
COMPUTE_BACKEND=cpu

# =============================================================================
# MEMORY AND LEARNING SYSTEMS
# =============================================================================

# -----------------------------------------------------------------------------
# Persistent Memory Configuration
# -----------------------------------------------------------------------------

# SQLite database path for long-term memory storage
# Required: No | Default: data/memory.db
AI_MEMORY_DB_PATH=data/memory.db

# Memory consolidation interval in seconds
# Required: No | Default: 300 (5 minutes)
# How often to consolidate short-term memory into long-term storage
AI_MEMORY_CONSOLIDATION_INTERVAL_S=300

# Maximum working memory buffer size (number of items)
# Required: No | Default: 1000
AI_MEMORY_WORKING_MAX_SIZE=1000

# Session memory TTL (time-to-live) in hours
# Required: No | Default: 24
# How long to keep session data before cleanup
AI_MEMORY_SESSION_TTL_HOURS=24

# -----------------------------------------------------------------------------
# Session Memory (Redis/DragonflyDB) - Optional
# -----------------------------------------------------------------------------

# Redis/DragonflyDB connection URL (optional - improves performance)
# Required: No | Default: None (uses in-memory fallback)
# If not set, session memory will use working memory only
# Examples:
#   redis://localhost:6379
#   redis://localhost:6379/0
#   redis://:password@localhost:6379/0
#   redis://username:password@localhost:6379
REDIS_URL=

# =============================================================================
# LLM PROVIDER CONFIGURATION (Optional - for LLM-powered features)
# =============================================================================
# Configure at least one provider for LLM-assisted decision-making and chat
# Providers are tried in order: OpenAI ‚Üí Azure ‚Üí DeepSeek ‚Üí Claude
# Only needed if COMPUTE_BACKEND=llm or for advanced chat features

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------

# OpenAI API key (get from: https://platform.openai.com/api-keys)
# Required: Only if using OpenAI | Default: None
OPENAI_API_KEY=

# OpenAI model to use
# Required: No | Default: gpt-5.1
# GPT-5 Series (Flagship): gpt-5.1, gpt-5-mini, gpt-5-nano
# GPT-4o Series: gpt-4o, gpt-4o-mini, gpt-4o-audio-preview
# o-Series (Reasoning - no temperature!): o1, o1-pro, o1-mini, o3, o3-mini, o4-mini
# Legacy: gpt-4-turbo, gpt-4, gpt-3.5-turbo
# Recommended: gpt-5.1 (most capable), gpt-4o-mini (cost-effective)
OPENAI_MODEL=gpt-5.1

# OpenAI API base URL (for custom endpoints)
# Required: No | Default: https://api.openai.com/v1
OPENAI_API_BASE=

# -----------------------------------------------------------------------------
# Azure OpenAI Configuration
# -----------------------------------------------------------------------------

# Azure OpenAI API key
# Required: Only if using Azure OpenAI | Default: None
AZURE_OPENAI_KEY=

# Azure OpenAI endpoint URL
# Required: Only if using Azure OpenAI | Default: None
# Example: https://your-resource.openai.azure.com/
AZURE_OPENAI_ENDPOINT=

# Azure OpenAI deployment name
# Required: Only if using Azure OpenAI | Default: gpt-4
# This is the deployment name you created in Azure Portal
AZURE_OPENAI_DEPLOYMENT=gpt-4

# Azure OpenAI API version
# Required: No | Default: 2024-02-01
AZURE_OPENAI_API_VERSION=2024-02-01

# -----------------------------------------------------------------------------
# DeepSeek Configuration (Cost-Effective Alternative)
# -----------------------------------------------------------------------------

# DeepSeek API key (get from: https://platform.deepseek.com/)
# Required: Only if using DeepSeek | Default: None
# Note: DeepSeek V3.2 is ~70% cheaper than OpenAI with comparable quality
DEEPSEEK_API_KEY=

# DeepSeek model to use
# Required: No | Default: deepseek-chat
# Options:
#   - deepseek-chat: V3.2 general chat model (recommended)
#   - deepseek-reasoner: V3.2 with enhanced reasoning/thinking mode
#   - deepseek-coder: Alias for deepseek-chat (backward compatibility)
# Recommended: deepseek-chat for general use, deepseek-reasoner for complex tasks
DEEPSEEK_MODEL=deepseek-chat

# DeepSeek API base URL
# Required: No | Default: https://api.deepseek.com/v1
DEEPSEEK_API_BASE=

# -----------------------------------------------------------------------------
# Claude (Anthropic) Configuration
# -----------------------------------------------------------------------------

# Anthropic API key (get from: https://console.anthropic.com/)
# Required: Only if using Claude | Default: None
ANTHROPIC_API_KEY=

# Claude model to use
# Required: No | Default: claude-sonnet-4-5
# Claude 4 Series (Latest):
#   - claude-sonnet-4-5: Recommended - balanced performance (supports thinking)
#   - claude-opus-4-1: Most capable (supports thinking)
#   - claude-opus-4-0: High capability
#   - claude-haiku-4-5: Fastest and most cost-effective
# Claude 3.5 Series:
#   - claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022
# Claude 3 Series (Legacy):
#   - claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
# Recommended: claude-sonnet-4-5 (best value), claude-opus-4-1 (most capable)
ANTHROPIC_MODEL=claude-sonnet-4-5

# Anthropic API base URL (for custom endpoints)
# Required: No | Default: https://api.anthropic.com
ANTHROPIC_API_BASE=

# -----------------------------------------------------------------------------
# Google AI (Gemini) Configuration - Optional
# -----------------------------------------------------------------------------

# Google AI API key (get from: https://makersuite.google.com/app/apikey)
# Required: Only if using Google AI | Default: None
GOOGLE_API_KEY=

# Google AI model to use
# Required: No | Default: gemini-pro
# Options: gemini-pro, gemini-pro-vision
GOOGLE_MODEL=gemini-pro

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLM) - Optional
# -----------------------------------------------------------------------------

# Ollama API base URL (for local LLM server)
# Required: Only if using Ollama | Default: None
# Example: http://localhost:11434
OLLAMA_API_BASE=

# Ollama model to use
# Required: Only if using Ollama | Default: llama2
# Must match model name in your Ollama installation
OLLAMA_MODEL=llama2

# =============================================================================
# ML DECISION ENGINE CONFIGURATION
# =============================================================================

# ML Decision Engine Settings
# Required: No | Default: 0.7
# Confidence threshold for accepting ML-based decisions (0.0 - 1.0)
ML_CONFIDENCE_THRESHOLD=0.7

# ML Strategy: ml_only, hybrid, ml_fallback
# Required: No | Default: hybrid
# - ml_only: Only use ML for decisions (may fail if low confidence)
# - hybrid: Blend ML and rule-based (recommended)
# - ml_fallback: Try ML first, fallback to rules if fails
ML_STRATEGY=hybrid

# ML Model Name
# Required: No | Default: decision_model
# Name of the ML model to use (must be trained)
ML_MODEL_NAME=decision_model

# Enable Learning
# Required: No | Default: true
# Whether to continuously learn from gameplay
ML_ENABLE_LEARNING=true

# ML Weight in Hybrid Mode (0.0 - 1.0)
# Required: No | Default: 0.6
# How much to weight ML decisions vs rules in hybrid mode
ML_WEIGHT=0.6

# =============================================================================
# LEARNING ENGINE CONFIGURATION
# =============================================================================

# Learning rate for strategy updates (0.0 - 1.0)
# Required: No | Default: 0.1
# Higher = faster learning but less stable, Lower = slower but more stable
AI_LEARNING_RATE=0.1

# Batch size for experience replay
# Required: No | Default: 20
# Number of experiences to learn from in each replay cycle
AI_LEARNING_BATCH_SIZE=20

# Outcome timeout in minutes
# Required: No | Default: 5
# Decisions older than this are marked as failed/timed out
AI_LEARNING_OUTCOME_TIMEOUT_MIN=5

# Experience replay interval in game ticks
# Required: No | Default: 1000
# How often to replay and learn from past experiences (every N ticks)
AI_LEARNING_REPLAY_INTERVAL_TICKS=1000

# Learning Model Type: random_forest, gradient_boosting, hist_gradient_boosting
# Required: No | Default: random_forest
# Type of machine learning model to use
LEARNING_MODEL_TYPE=random_forest

# Minimum samples required for training
# Required: No | Default: 100
# Minimum number of samples before model training begins
LEARNING_MIN_SAMPLES=100

# Test split ratio (0.0 - 1.0)
# Required: No | Default: 0.2
# Fraction of data to use for testing (20%)
LEARNING_TEST_SPLIT=0.2

# Random state for reproducibility
# Required: No | Default: 42
# Seed for random number generator
LEARNING_RANDOM_STATE=42

# =============================================================================
# EQUIPMENT MANAGEMENT CONFIGURATION
# =============================================================================

# Auto-optimize equipment
# Required: No | Default: true
# Automatically switch equipment based on situation
EQUIPMENT_AUTO_OPTIMIZE=true

# Equipment check interval (in ticks)
# Required: No | Default: 100
# How often to check and optimize equipment (every N ticks)
EQUIPMENT_CHECK_INTERVAL_TICKS=100

# =============================================================================
# JOB ADVANCEMENT CONFIGURATION
# =============================================================================

# Job Test Max Retries
# Required: No | Default: 3
# Maximum number of retry attempts for job test failures
JOB_TEST_MAX_RETRIES=3

# Job Test Timeout (minutes)
# Required: No | Default: 30
# Maximum time allowed for completing job test
JOB_TEST_TIMEOUT_MINUTES=30

# =============================================================================
# NAVIGATION CONFIGURATION
# =============================================================================

# Navigation Preference: fastest, cheapest, safest, balanced
# Required: No | Default: balanced
# - fastest: Prioritize speed (may use expensive warps)
# - cheapest: Minimize cost (walk more, use free portals)
# - safest: Avoid dangerous areas (may take longer)
# - balanced: Balance of all factors (recommended)
NAVIGATION_PREFERENCE=balanced

# Navigation Cache Size
# Required: No | Default: 1000
# Number of navigation routes to cache in memory
NAVIGATION_CACHE_SIZE=1000

# =============================================================================
# ECONOMY CONFIGURATION
# =============================================================================

# Economy Server Type: low_rate, mid_rate, high_rate
# Required: No | Default: mid_rate
# Affects pricing strategies and economy decisions
# - low_rate: 1x-5x rates (conservative trading)
# - mid_rate: 5x-50x rates (balanced trading)
# - high_rate: 50x+ rates (aggressive trading)
ECONOMY_SERVER_TYPE=mid_rate

# =============================================================================
# QUEST SYSTEM CONFIGURATION
# =============================================================================

# Quest Database Path
# Required: No | Default: data/quests/quest_database.yml
# Path to the quest database file
QUEST_DATABASE_PATH=data/quests/quest_database.yml

# Auto-suggest quests
# Required: No | Default: true
# Automatically suggest appropriate quests for character level
QUEST_AUTO_SUGGEST=true

# =============================================================================
# SECURITY AND API RATE LIMITING
# =============================================================================

# API rate limit (requests per minute)
# Required: No | Default: 60
# Prevents excessive API calls to LLM providers
API_RATE_LIMIT_PER_MINUTE=60

# API daily budget limit in USD (0 = unlimited)
# Required: No | Default: 0
# Set this to prevent unexpected API costs
API_DAILY_BUDGET_LIMIT_USD=0

# Enable response caching to reduce API calls
# Required: No | Default: true
API_ENABLE_RESPONSE_CACHE=true

# Cache TTL in seconds
# Required: No | Default: 3600 (1 hour)
API_CACHE_TTL_SECONDS=3600

# =============================================================================
# ANTI-DETECTION CONFIGURATION
# =============================================================================

# Enable anti-detection features
# Required: No | Default: true
# Randomizes timing and behavior to appear more human-like
ANTI_DETECTION_ENABLED=true

# Paranoia level: low, medium, high, extreme
# Required: No | Default: medium
# Higher levels add more randomization but may reduce efficiency
ANTI_DETECTION_PARANOIA_LEVEL=medium

# Timing variance in milliseconds
# Required: No | Default: 150
# Random delay added to actions (Gaussian distribution)
ANTI_DETECTION_TIMING_VARIANCE_MS=150

# Maximum continuous play hours
# Required: No | Default: 4
# Bot will take breaks after this duration
ANTI_DETECTION_MAX_CONTINUOUS_HOURS=4

# Daily maximum play hours
# Required: No | Default: 12
# Total play time limit per 24-hour period
ANTI_DETECTION_DAILY_MAX_HOURS=12

# Enable GM detection and stealth mode
# Required: No | Default: true
# Automatically enters stealth mode when GM is detected nearby
ANTI_DETECTION_GM_DETECTION_ENABLED=true

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Enable performance profiling
# Required: No | Default: false
# Useful for debugging performance issues
AI_ENABLE_PROFILING=false

# Profiling output directory
# Required: No | Default: profiles
AI_PROFILE_OUTPUT_DIR=profiles

# Enable metrics collection (Prometheus-compatible)
# Required: No | Default: false
AI_ENABLE_METRICS=false

# Metrics export port (if metrics enabled)
# Required: No | Default: 9090
AI_METRICS_PORT=9090

# Maximum memory usage in MB (0 = unlimited)
# Required: No | Default: 512
# Process will try to stay under this limit
AI_MAX_MEMORY_MB=512

# =============================================================================
# OPENKORE INTEGRATION
# =============================================================================

# OpenKore control directory path
# Required: No | Default: None
# Path to OpenKore's control directory for reading config files
OPENKORE_CONTROL_DIR=

# OpenKore plugin directory path
# Required: No | Default: None
# Path to OpenKore's plugins directory
OPENKORE_PLUGIN_DIR=

# Enable OpenKore state synchronization
# Required: No | Default: true
# Keeps AI state in sync with OpenKore's game state
OPENKORE_STATE_SYNC_ENABLED=true

# =============================================================================
# END OF CONFIGURATION
# =============================================================================
# For more information, see:
# - README.md: https://github.com/OpenKore/openkore
# - Documentation: docs/GODTIER-AI-SPECIFICATION.md
# - AI Sidecar README: ai_sidecar/README.md